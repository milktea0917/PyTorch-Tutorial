# âœ… 1. æ¨¡æ“¬è³‡æ–™ï¼šç·šæ€§å‡½æ•¸åŠ é›œè¨Š
import torch
from torch.utils.data import TensorDataset, DataLoader

# å»ºç«‹è³‡æ–™
X = torch.linspace(0, 10, steps=100).reshape(-1, 1)
y = 3 * X + 2 + torch.randn_like(X) * 2  # y = 3x + 2 + noise

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=16, shuffle=True)

# âœ… 2. å»ºç«‹æ¨¡å‹ã€Loss å‡½æ•¸èˆ‡ Optimizer
import torch.nn as nn

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = nn.Linear(1, 1).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# âœ… 3. è¨“ç·´è¿´åœˆï¼šæ¨™æº–äº”æ­¥é©Ÿ
epochs = 50

for epoch in range(epochs):
    running_loss = 0.0
    for batch_X, batch_y in loader:
        # è³‡æ–™æ¬åˆ° GPU
        batch_X = batch_X.to(device)
        batch_y = batch_y.to(device)

        # Step 1: forward
        preds = model(batch_X)

        # Step 2: compute loss
        loss = criterion(preds, batch_y)

        # Step 3: æ¸…é™¤ä¸Šä¸€è¼ªçš„æ¢¯åº¦
        optimizer.zero_grad()

        # Step 4: backward
        loss.backward()

        # Step 5: æ›´æ–°åƒæ•¸
        optimizer.step()

        running_loss += loss.item()

    print(f"[Epoch {epoch+1}] Loss: {running_loss:.4f}")

# âœ… 4. è¦–è¦ºåŒ–çµæœï¼ˆå¯é¸é€²éšï¼‰
import matplotlib.pyplot as plt

# é æ¸¬
model.eval()
with torch.no_grad():
    X_cpu = X.to(device)
    y_pred = model(X_cpu).cpu()

plt.scatter(X, y, label='True')
plt.plot(X, y_pred, color='red', label='Prediction')
plt.legend()
plt.title("Linear Regression Fit")
plt.show()

# âœ… 5. æŒ‘æˆ°ç·´ç¿’ï¼ˆOptionalï¼‰
## å˜—è©¦ä½¿ç”¨ Adam optimizer ä¸¦è§€å¯Ÿ loss æ”¶æ–‚æ˜¯å¦æ›´å¿«

### ğŸ”¶ 1. è³‡æ–™ç”¢ç”Ÿèˆ‡ Datasetã€DataLoader
import torch
from torch.utils.data import TensorDataset, DataLoader
X = torch.linspace(0, 10, steps=100).reshape(-1, 1)
y = 3 * X + 2 + torch.randn_like(X) * 2
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=4, shuffle=True)

### ğŸ”¶ 2. å»ºç«‹æ¨¡å‹ã€Loss èˆ‡ Optimizer
import torch.nn as nn
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = nn.Linear(1,1).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)

### ğŸ”¶ 3. è¨“ç·´è¿´åœˆ
epochs = 50
for epoch in range(epochs):
  running_loss = 0.0
  for batch_X, batch_y in loader:

    batch_X = batch_X.to(device)
    batch_y = batch_y.to(device)

    preds = model(batch_X)
    loss = criterion(preds, batch_y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    running_loss += loss.item()
  print(f"[Epoch {epoch+1}] Lossï¼š{running_loss:.4f}")

# ANSWERï¼šLOSSæ”¶æ–‚æ²’æœ‰æ›´å¿«ï¼Œä½†ä¼¼ä¹æ›´ç‚ºå¹³æ»‘ (âœ”ï¸ é€™å€‹è§€å¯Ÿå…¶å¯¦é —ç²¾æº–ï¼Adam å„ªé»æœ¬ä¾†å°±ä¸æ˜¯ã€Œå¿«ã€è€Œæ˜¯ã€Œç©©å®šã€èˆ‡ã€Œè‡ªå‹•èª¿æ•´å­¸ç¿’ç‡ã€ï¼Œé€šå¸¸åœ¨å™ªè²è¼ƒå¤§æˆ– loss æ›²ç·šèµ·ä¼å¤§çš„è³‡æ–™ä¸Šæœƒæ¯”è¼ƒæ˜é¡¯ã€‚)
# batch_size æ”¹å¤§ä¸€é»ï¼ˆä¾‹å¦‚ 16ï¼‰ï¼Œå¯èƒ½æ›´èƒ½çœ‹å‡º Adam èˆ‡ SGD çš„å·®ç•°ã€‚

## å˜—è©¦ç”¨ 3 å±¤ç¥ç¶“ç¶²è·¯å–ä»£ Linear æ¨¡å‹ï¼šLinear(1,16) â†’ ReLU â†’ Linear(16,1)
### ğŸ”¶ 4. è³‡æ–™ç”¢ç”Ÿèˆ‡ Datasetã€DataLoader
import torch
from torch.utils.data import TensorDataset, DataLoader
X = torch.linspace(0, 10, steps=100).reshape(-1, 1) # (100,) --> (100, 1)
y = 3 * X + 2 + torch.randn_like(X) * 2  # y = 3x + 2 + noise
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=4, shuffle=True)

### ğŸ”¶ 5. å»ºç«‹æ¨¡å‹ã€Loss èˆ‡ Optimizer
import torch.nn as nn
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
class SimpleNeuralNetwork(nn.Module):
  def __init__(self):
      super(SimpleNeuralNetwork, self).__init__()
      # Define the layers of your network in the __init__ method
      self.fc1 = nn.Linear(1, 16)  # Input layer (1 features) to hidden layer (16 neurons)
      self.fc2 = nn.Linear(16, 1) # Hidden layer (16 neurons) to another hidden layer (1 neurons)
      
  def forward(self, x):
      # Define the forward pass, specifying how data flows through the layers
      x = self.fc2(F.relu(self.fc1(x))) # Apply ReLU activation after the first linear layer
      return x

model = SimpleNeuralNetwork().to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)

### ğŸ”¶ 6. è¨“ç·´è¿´åœˆ
epochs = 50
for epoch in range(epochs):
  running_loss = 0.0
  for batch_X, batch_y in loader:

    batch_X = batch_X.to(device)
    batch_y = batch_y.to(device)

    preds = model(batch_X)
    loss = criterion(preds, batch_y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    running_loss += loss.item()
  
  print(f"[Epoch {epoch+1}] Lossï¼š{running_loss:.4f}")

## å˜—è©¦ç´€éŒ„æ¯å€‹ epoch çš„ loss ä¸¦ç”¨åœ–ç•«å‡º loss æ›²ç·šï¼ˆç”¨ matplotlibï¼‰
### ğŸ”¶ 7. è³‡æ–™ç”¢ç”Ÿèˆ‡ Datasetã€DataLoader
import torch
from torch.utils.data import TensorDataset, DataLoader
X = torch.linspace(0, 10, steps=100).reshape(-1, 1) # (100,) --> (100, 1)
y = 3 * X + 2 + torch.randn_like(X) * 2  # y = 3x + 2 + noise
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=4, shuffle=True)

### ğŸ”¶ 8. å»ºç«‹æ¨¡å‹ã€Loss èˆ‡ Optimizer
import torch.nn as nn
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class SimpleNeuralNetwork(nn.Module):
  def __init__(self):
      super(SimpleNeuralNetwork, self).__init__()
      # Define the layers of your network in the __init__ method
      self.fc1 = nn.Linear(1, 16)  # Input layer (1 features) to hidden layer (16 neurons)
      self.fc2 = nn.Linear(16, 1) # Hidden layer (16 neurons) to another hidden layer (1 neurons)
      
  def forward(self, x):
      # Define the forward pass, specifying how data flows through the layers
      x = self.fc2(F.relu(self.fc1(x))) # Apply ReLU activation after the first linear layer
      return x

model = SimpleNeuralNetwork().to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)

### ğŸ”¶ 9. è¨“ç·´è¿´åœˆ
import matplotlib.pyplot as plt
Loss_List = []

epochs = 50
for epoch in range(epochs):
  running_loss = 0.0
  for batch_X, batch_y in loader:

    batch_X = batch_X.to(device)
    batch_y = batch_y.to(device)

    preds = model(batch_X)
    loss = criterion(preds, batch_y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    running_loss += loss.item()

  print(f"[Epoch {epoch+1}] Lossï¼š{running_loss:.4f}")
  # Loss_List.append(running_loss) # ç¸½å’Œ
  Loss_List.append(running_loss / len(loader))  # å¹³å‡ loss (âœ…å»ºè­° - å¹³å‡ (ä¸éš¨batchå¢åŠ è®Šå¤§))
  
plt.scatter(range(epochs), Loss_List, label="Loss per epoch") (âœ…å»ºè­° - åŠ å…¥label)
plt.plot(range(epochs), Loss_List, color='red')
plt.title("Training Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
