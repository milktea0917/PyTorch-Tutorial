# âœ… 1. å»ºç«‹å¸¶æœ‰ requires_grad çš„ Tensor
import torch

x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x ** 2 + 2 * x + 1

print("y:", y)

# âœ… 2. ä½¿ç”¨ .backward() è¨ˆç®—æ¢¯åº¦

## å› ç‚º y æ˜¯å‘é‡ï¼Œæˆ‘å€‘è¦çµ¦ä¸€å€‹åŒå‹çš„æ¬Šé‡å‘é‡ä¾†å‘Šè¨´ PyTorch å¦‚ä½•åŠ ç¸½å®ƒ
grad_output = torch.tensor([1.0, 1.0])
y.backward(gradient=grad_output)

print("x çš„æ¢¯åº¦:", x.grad)

## ğŸ“Œ èªªæ˜ï¼š
## y = xÂ² + 2x + 1 â†’ dy/dx = 2x + 2
## æ‰€ä»¥å°æ‡‰ä½ç½®æœƒæ˜¯ [2*2 + 2, 2*3 + 2] = [6, 8]

# âœ… 3. å¸¸è¦‹é‹ç®—è‡ªå‹•å¾®åˆ†å¯¦ä¾‹ï¼ˆæ¨™é‡ backward ä¸éœ€æŒ‡å®š gradientï¼‰

a = torch.tensor(5.0, requires_grad=True)
b = a ** 3 + 2 * a
b.backward()
print("db/da:", a.grad)  # 3aÂ² + 2 = 3*25 + 2 = 77

# âœ… 4. åœç”¨æ¢¯åº¦è¿½è¹¤çš„æ–¹æ³•

w = torch.randn(2, 2, requires_grad=True)

## æ–¹æ³• 1ï¼šdetach()
no_grad_w = w.detach()
print("æ¢¯åº¦è¿½è¹¤é—œé–‰:", no_grad_w.requires_grad)

## æ–¹æ³• 2ï¼šwith torch.no_grad()
with torch.no_grad():
    z = w * 2
print("z requires_grad:", z.requires_grad)

# âœ… 5. æ¸…é™¤æ¢¯åº¦

x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x * 2
y.backward(gradient=torch.ones_like(x))

print("ç¬¬ä¸€æ¬¡æ¢¯åº¦:", x.grad)

x.grad.zero_()  # é‡è¨­
y = x * 3
y.backward(gradient=torch.ones_like(x))
print("ç¬¬äºŒæ¬¡æ¢¯åº¦:", x.grad)

# âœ… 6. æŒ‘æˆ°ç·´ç¿’ï¼ˆOptionalï¼‰
## è¨­å®šè®Šæ•¸ xï¼Œè¨ˆç®—å‡½æ•¸ f(x) = sin(x) * x^2 çš„æ¢¯åº¦ï¼Œè¼¸å‡º df/dx
x = torch.randn(2,3, requires_grad=True) # âœ… è¦è¨˜å¾—requires_gradï¼Œæ‰èƒ½è¨ˆç®—backward()
y = torch.sin(x) * (x**2)
y.sum().backward() # è¨ˆç®—xçš„æ¢¯åº¦
print("df/dx:", x.grad)
### why y.sum().backward()? --> âœ… https://discuss.pytorch.org/t/loss-backward-raises-error-grad-can-be-implicitly-created-only-for-scalar-outputs/12152

## å»ºç«‹ä¸€å€‹å«æœ‰ 2 å±¤æ“ä½œçš„è¨ˆç®—åœ–ï¼ˆå¦‚ï¼šå…ˆå¹³æ–¹å†ä¹˜ä»¥å¸¸æ•¸ï¼‰ï¼Œä¸¦æŸ¥çœ‹æ¯ä¸€å±¤çš„æ¢¯åº¦
x = torch.randn(2,3, requires_grad=True)
y = x ** 3 + (2*x)**2 + 4
y.sum().backward()
print("df/dx:", x.grad)

## ä½¿ç”¨ with torch.no_grad() åŒ…ä½ä¸€æ®µé‹ç®—ï¼Œä¸¦èªªæ˜ç‚ºä»€éº¼é€™æ¨£åšï¼ˆæç¤ºï¼šinference éšæ®µï¼‰
with torch.no_grad():
  x = torch.randn(2,3) # âœ… GPTï¼šè¨˜å¾—ä¸è¦è¨­requires_grad=Trueï¼Œé€™æ¨£æ­é…no_grad()æ•ˆç‡æœƒæ›´é«˜ã€‚
  y = torch.sin(x) * (x**2)
# ANSWERï¼šå› ç‚ºåœ¨inferenceéšæ®µï¼Œæˆ‘å€‘åªæ˜¯è¦modelåŸºæ–¼è¨“ç·´å¥½çš„weightç”¢ç”Ÿoutputï¼Œä¸¦æ²’æœ‰è¦å›å»æ›´æ–°modelçš„åƒæ•¸ï¼Œæ‰€ä»¥ä¸éœ€è¦è¨ˆç®—gradient
# âœ… GPTï¼š å®Œå…¨äº†è§£ inference éšæ®µä¸éœ€æ¢¯åº¦ã€å¯ç¯€çœè¨˜æ†¶é«”èˆ‡è¨ˆç®—è³‡æºã€‚
