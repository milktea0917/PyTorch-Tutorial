import torch 
from torch.utils.data import TensorDataset, DataLoader 
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#  產生簡單 2D 二分類資料（可視化好用）
X, y = make_classification(n_samples=500, n_features=2, n_redundant=0,
                          n_informative=2, n_classes=2, random_state=42) 

#  轉為 Tensor
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.long) # CrossEntrophy 需要long型別

# 分 train / test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
train_ds = TensorDataset(X_train, y_train)
test_ds = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=32)

# Create model
class mlp_model(nn.Module):
    def __init__(self) -> None:
        super(mlp_model, self).__init__()
        self.fc1 = nn.Linear(2,16)
        self.fc2 = nn.Linear(16,2)
        self.dropout = nn.Dropout(p=0.2)

    def forward(self, x):
        # x = self.fc2(F.relu(self.fc1(x))) # Test Accuracy：91.0%
        # x = self.fc2(self.dropout(F.relu(self.fc1(x)))) # Test Accuracy：91.0%
        # x = self.fc2(F.sigmoid(self.fc1(x))) # Test Accuracy：88.0%
        # x = self.fc2(self.dropout(F.sigmoid(self.fc1(x)))) # Test Accuracy：88.0%
        # x = self.fc2(F.tanh(self.fc1(x))) # Test Accuracy：89.0%
        x = self.fc2(self.dropout(F.tanh(self.fc1(x)))) # Test Accuracy：89.0%
        return x
    
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
model = mlp_model().to(device)

# Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# train_loop
epochs = 50
train_losses = []

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        logits = model(batch_X)
        loss = criterion(logits, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    train_losses.append(avg_loss)
    print(f"[Epoch {epoch+1}] Loss: {avg_loss:.4f}")

# draw Loss Curve
plt.plot(train_losses, label='train_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.scatter(X[:,0], X[:,1], c=y)
plt.legend()
plt.show()
plt.savefig("loss_curve_tanh+dropout.png")
plt.close()

# evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch_X, batch_y in test_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        logits = model(batch_X)
        pred = torch.argmax(logits, dim=1)
        correct += (pred==batch_y ).sum().item()
        total += batch_y.size(0)

acc = correct / total
print(f"Test Accuracy: {acc * 100:.2f}%")

# decision boundry
xx, yy = torch.meshgrid(torch.linspace(-3, 3, 100), torch.linspace(-3, 3, 100), indexing='xy')
grid = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1).to(device)
with torch.no_grad():
    logits = model(grid)
    preds = torch.argmax(logits, dim=1).cpu().reshape(100, 100)

# 畫出decision boundary
plt.contourf(xx, yy, preds, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
plt.title("Decision Boundary")
plt.show()
plt.savefig("decision_boundary_tanh+dropout.png")
plt.close()
