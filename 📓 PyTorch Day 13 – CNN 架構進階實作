import torch 
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torchvision.utils import make_grid
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
from torchinfo import summary
import os
import re

# prepare MNIST Data 
## Normal Version
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
## Data Augmentation
train_transform = transforms.Compose([
    transforms.RandomRotation(10),
    transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_data = datasets.MNIST(root="./data", train=True, transform=train_transform, download=True)
train_data, val_data = random_split(train_data, [50000, 10000], generator=torch.Generator().manual_seed(42)) # fix random seed 
test_data = datasets.MNIST(root="./data", train=False, transform=transform) # 不需要對test做 Augmentation，這樣子不公平

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(val_data, batch_size=1000)
test_loader = DataLoader(test_data, batch_size=1000)

# create model
class CNN_A_Baseline(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(8)
        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(16)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(16 * 7 * 7, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = x.view(-1, 16 * 7 * 7) # 除了batch之外，都壓縮成一維
        x = self.fc1(x)
        return x
    
class CNN_B_Deep(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(8)
        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(16)
        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(32)
        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(64)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 *7 *7, 10)
        self.dropout = nn.Dropout(p=0.3)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool(x)
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = self.pool(x)
        x = x.view(x.size(0), -1)# 除了batch之外，都壓縮成一維
        x = self.dropout(x) # dropout 應在 fully connected layer 前 
        x = self.fc1(x)
        return x
    
class CNN_C_WideKernel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)
        self.bn1 = nn.BatchNorm2d(8)
        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2)
        self.bn2 = nn.BatchNorm2d(16)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(16 * 7 * 7, 10)
        self.dropout = nn.Dropout(p=0.3)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool(x)
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool(x)
        x = x.view(-1, 16 * 7 * 7) # 除了batch之外，都壓縮成一維
        x = self.dropout(x) # dropout 應在 fully connected layer 前 
        x = self.fc1(x)
        return x
    
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
# model = CNN_A_Baseline().to(device)
# model = CNN_B_Deep().to(device) # if dropout added
model = CNN_C_WideKernel().to(device) # if dropout added

# 比較模型差異
summary(CNN_A_Baseline(), input_size=(1, 1, 28, 28))
summary(CNN_B_Deep(), input_size=(1, 1, 28, 28))
summary(CNN_C_WideKernel(), input_size=(1, 1, 28, 28))

# 實作一個 2~3 層 CNN 模型，列出每層 output shape
# input_size = (64,1,28,28)
# after self.conv1, size --> (64,8,28,28)
# after MaxPool2d, size --> (64,8,14,14)
# after self.conv2, size --> (64,16,14,14)
# after MaxPool2d, size --> (64,16,7,7)
# after x.view, size --> (64,784)
# after fc1, size --> (64,10)

# optimizer 
criterion = nn.CrossEntropyLoss()
# optimizer = torch.optim.SGD(model.parameters(), lr= 0.01, weight_decay=1e-4)
optimizer = torch.optim.Adam(model.parameters(), lr= 0.01, weight_decay=1e-4)

# train loop
os.makedirs("model_checkpoint", exist_ok=True) # for saving model_checkpoint
os.makedirs("runs", exist_ok=True) # for saving tensorboard
writer = SummaryWriter(log_dir=f"runs/MNIST_CNN_{model.__class__.__name__}_{optimizer.__class__.__name__}")

epochs = 15
train_losses = []
val_losses = []
val_accs = []

for epoch in range(epochs):
    model.train()
    batch_loss = 0
    for batch_X, batch_y in train_loader:
        
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        
        logits = model(batch_X)
        loss = criterion(logits, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        batch_loss += loss.item()

    avg_loss = batch_loss / len(train_loader)

    # if better than the exists checkpoint, then save
    val_loss = 0
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for batch_X, batch_y in val_loader:
            
            batch_X, batch_y  = batch_X.to(device), batch_y.to(device)
            logits  = model(batch_X)

            # if loss:
            batch_val_loss = criterion(logits, batch_y)
            val_loss += batch_val_loss.item()

            # if accuracy:
            pred = torch.argmax(logits, dim=1)
            correct += (pred==batch_y).sum().item()
            total += batch_y.size(0)
    
    avg_val_loss = val_loss / len(val_loader)
    
    val_acc = correct / total
    writer.add_scalar('Accuracy/val', val_acc, epoch)
    
    # if better than exist model, save model_checkpoint
    # GPT：其中一項改善就儲存
    if len(val_losses) == 0 or ( avg_val_loss < val_losses[-1] or val_acc > val_accs[-1] ):
        torch.save({'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': loss.item()}, f"model_checkpoint/checkpoint_{epoch}.pt")
        
    print(f"[Epoch {epoch+1}] train_loss: {avg_loss * 100:.2f}% val_loss: {avg_val_loss* 100:.2f}% Test Accuracy: {val_acc * 100:.2f}%")
    
    train_losses.append(avg_loss)
    val_losses.append(avg_val_loss)
    val_accs.append( val_acc )

    # tensorboard 紀錄loss
    writer.add_scalar('Loss/train', avg_loss, epoch+1)
    writer.add_scalar('Loss/val', avg_val_loss, epoch+1)

    # tensorboard 紀錄權重、紀錄梯度
    for name, param in model.named_parameters():
        if param.requires_grad:
            writer.add_histogram(f"{name}_weights", param.data.cpu().numpy(), epoch+1)
            writer.add_histogram(f"{name}_grads", param.grad.data.cpu().numpy(), epoch+1)

# evaluation
model_checkpoint_path = "./model_checkpoint"
assert os.path.exists(model_checkpoint_path)==True

model_list = os.listdir(model_checkpoint_path)
model_list = sorted(
    [f for f in model_list if f.startswith("checkpoint_")],
    key=lambda x:int(re.findall(r"\d+", x)[0])
)

model_evaluate_result = {}
for i in model_list:
    
    correct = 0
    total = 0
    print(f"Loading model checkpoint: {i}")
    checkpoint = torch.load(os.path.join(model_checkpoint_path, i))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            logits = model(batch_X)
            pred = torch.argmax(logits, dim=1)
            correct += (pred==batch_y).sum().item()
            total += batch_y.size(0)

    acc = correct / total
    print(f"Test Accuracy: {acc * 100:.2f}% in model_checkpoint: {i}")
    writer.add_scalar('Accuracy/test', acc, int(re.findall(r"\d+", i)[0]))
    model_evaluate_result[i] = acc

writer.close()

# find the best model from evaluate result
best_model = max(model_evaluate_result, key=model_evaluate_result.get)
best_acc = model_evaluate_result[best_model]

# rename
best_model_path = os.path.join(model_checkpoint_path, best_model)
best_model_rename_path = os.path.join(model_checkpoint_path, 'best_model.pt')

if os.path.exists(best_model_rename_path):
    print(f"There exists a best_model, the script is gonig to replace.")
    os.remove(best_model_rename_path)

os.rename(best_model_path, best_model_rename_path)
print(f"✅ Renamed best model '{best_model}' → 'best_model.pt' (Acc: {best_acc:.4f})")

# Experiment Results (Test Accuracy)
# CNN_A_Baseline_SGD：0.9867
# CNN_A_Baseline_Adam：0.9886
# CNN_B_Deep：0.9921
# CNN_C_WideKernel：0.9902
