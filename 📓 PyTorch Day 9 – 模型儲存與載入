import torch 
from torch.utils.data import TensorDataset, DataLoader 
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os 
import re

#  產生簡單 2D 二分類資料（可視化好用）
X, y = make_classification(n_samples=500, n_features=2, n_redundant=0,
                          n_informative=2, n_classes=2, random_state=42) 

#  轉為 Tensor
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.long) # CrossEntrophy 需要long型別

# 分 train / test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
train_ds = TensorDataset(X_train, y_train)
test_ds = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=32)

# decision boundry data
xx, yy = torch.meshgrid(torch.linspace(-3, 3, 100), torch.linspace(-3, 3, 100), indexing='xy')

# Create model
class MLPModel(nn.Module):
    def __init__(self) -> None:
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(2,16)
        self.fc2 = nn.Linear(16,2)
        self.dropout = nn.Dropout(p=0.2)

    def forward(self, x):
        # x = self.fc2(F.relu(self.fc1(x))) # Test Accuracy：91.0%
        x = self.fc2(self.dropout(F.relu(self.fc1(x)))) # Test Accuracy：91.0%
        # x = self.fc2(F.sigmoid(self.fc1(x))) # Test Accuracy：88.0%
        # x = self.fc2(self.dropout(F.sigmoid(self.fc1(x)))) # Test Accuracy：88.0%
        # x = self.fc2(F.tanh(self.fc1(x))) # Test Accuracy：89.0%
        # x = self.fc2(self.dropout(F.tanh(self.fc1(x)))) # Test Accuracy：89.0%
        return x
    
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
model = MLPModel().to(device)

# decision boundry data to device
grid = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1).to(device)

# Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# train_loop
os.makedirs("model_checkpoint", exist_ok=True) # for saving model_checkpoint
epochs = 10
train_losses = []

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        logits = model(batch_X)
        loss = criterion(logits, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)

    if len(train_losses)==0 or avg_loss < train_losses[-1]:
        # torch.save(model.state_dict(), f"model_checkpoint/checkpoint_{epoch}.pt")
        torch.save({'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': loss.item() }, f"model_checkpoint/checkpoint_{epoch}.pt")

         # decision boundry
        with torch.no_grad():
            model.eval()
            logits = model(grid)
            preds = torch.argmax(logits, dim=1).cpu().reshape(100, 100)

        plt.contourf(xx, yy, preds, alpha=0.3)
        plt.title("Decision Boundary")
        plt.savefig(f"decision_boundary_train_checkpoint_{epoch}.png")
        plt.show()
        plt.close()

    train_losses.append(avg_loss)
    # print(f"[Epoch {epoch+1}] Loss: {avg_loss:.4f}")

# draw Loss Curve
plt.plot(train_losses, label='train_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.scatter(X[:,0], X[:,1], c=y)
plt.legend()
plt.savefig("loss_curve_relu+dropout.png")
plt.show()
plt.close()

# evaluation
model_checkpoint_path = "./model_checkpoint/"

assert os.path.exists(model_checkpoint_path)==True
model_list = os.listdir(model_checkpoint_path)
model_list = sorted(
    [f for f in model_list if f.startswith("checkpoint_")],
    key=lambda x: int(re.findall(r"\d+", x)[0])
)
model_evaluate_result = {}

for i in model_list:

    correct = 0
    total = 0
    print(f"Loading model checkpoint：{i}")
    checkpoint = torch.load(os.path.join(model_checkpoint_path, i))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            logits = model(batch_X)
            pred = torch.argmax(logits, dim=1)
            correct += (pred==batch_y ).sum().item()
            total += batch_y.size(0)

        # decision boundry
        logits = model(grid)
        preds = torch.argmax(logits, dim=1).cpu().reshape(100, 100)

        plt.contourf(xx, yy, preds, alpha=0.3)
        plt.title("Decision Boundary")
        plt.savefig(f"decision_boundary_evaluate_{i}.png")
        plt.show()
        plt.close()

    acc = correct / total
    print(f"Test Accuracy: {acc * 100:.2f}% in model_checkpoint:{i}")
    model_evaluate_result[i] = acc
